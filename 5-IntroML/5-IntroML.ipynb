{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8fa137",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br/>\n",
    "<img src=\"images/cd-logo-blue-600x600.png\" alt=\"\" width=\"130px\" align=\"left\"/>\n",
    "<img src=\"images/cd-logo-blue-600x600.png\" alt=\"\" width=\"130px\" align=\"right\"/>\n",
    "<div align=\"center\">\n",
    "<h2>Bootcamp Data Science - Módulo 2</h2><br/>\n",
    "<h1>Introducción a Machine Learning</h1>\n",
    "<br/><br/>\n",
    "    <b>Instructor Principal:</b> Patricio Olivares polivares@codingdojo.cl <br/>\n",
    "    <b>Instructor Asistente:</b> Jesús Ortiz jortiz@codingdojo.cl<br/><br/>\n",
    "    <b>Coding Dojo</b>\n",
    "</div>\n",
    "<br>\n",
    "Fuente: \"Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19951c3c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# ¿Qué es Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb6b2b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Machine Learning (de ahora en adelante, **ML**), se define como el conjunto de técnicas a través de los cuales un computador puede aprender de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5bd22",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/diagrama.png\" alt=\"\" width=\"800px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af19a90",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# ¿Por qué utilizar Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d420b9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Ej. Creación de un filtro de spam para correos electrónicos\n",
    "\n",
    "- Aproximación tradicional\n",
    "\n",
    "<img src=\"images/withoutML.png\" alt=\"\" width=\"500px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c98f60",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# ¿Por qué utilizar Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ea1a7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Ej. Creación de un filtro de spam para correos electrónicos\n",
    "\n",
    "- Aproximación de ML\n",
    "\n",
    "<img src=\"images/withML.png\" alt=\"\" width=\"500px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612a9d9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Tipos de algoritmos de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6180672",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Existen distintas clasificaciones\n",
    "\n",
    "- Algoritmos entrenados con o sin supervisión humana:\n",
    "    * Supervisados\n",
    "    * No supervisados\n",
    "    * Semi supervisados\n",
    "    * Por refuerzo\n",
    "- Algoritmos de aprendizaje incremental:\n",
    "    * Online\n",
    "    * Offline/por lote (batch)\n",
    "- Algoritmos de detección de patrones o de comparación:\n",
    "    * Basados en instancia\n",
    "    * Basados en modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80c6651",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Bibliotecas para ML python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e5be8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/bibliotecas.png\" alt=\"\" width=\"800px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c1d880",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e406f16",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/ml-libraries/scikit-learn-logo-notext.png\" alt=\"\" width=\"200px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b3ed9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Biblioteca para ML de código libre\n",
    "- Biblioteca para Python\n",
    "- Soporta múltiples algoritmos tanto supervisados como no supervisados\n",
    "-  Si está trabajando en su entorno local, previo a su importación debe ser **instalada** \n",
    "    - ```pip install scikit-learn``` si usa ```pip```\n",
    "    - ```conda install scikit-learn``` si usa ```conda```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678c94f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Scikit-Learn: Utilización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4884df81",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Todo los objetos Scikit-Learn son **consistentes** entre sí\n",
    "* **Estimadores:** \n",
    "    - Método encargado de la **estimación** de los parámetros asociados al algoritmo. \n",
    "    - Objetos Scikit-Learn incluyen el método *fit()*, cuya entrada es el set de datos utilizados para el cálculo de los parámetros.\n",
    "* **Transformadores:** \n",
    "    - Método encargado de la **transformación** de datos en base a los parámetros calculados por método *fit()*. \n",
    "    - Objetos Scikit-Learn incluyen el método *transform()*, que recibe con los datos a transformar.\n",
    "    - Método *fit_transform()* permite realizar ambos pasos con un solo método.\n",
    "* **Predictores:** \n",
    "    - Método encargado de hacer **predicciones** en base a datos de entrada.\n",
    "    - Objetos Scikit-Learn incluyen el método *predict()*, para realizar predicciones en base a parámetros de entrada.\n",
    "        \n",
    "Cuáles de estos métodos están presentes, dependerá del tipo algoritmo a utilizar.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a78bca0",
   "metadata": {},
   "source": [
    "# Tipos de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defcbe3c",
   "metadata": {},
   "source": [
    "- Cada set de datos puede poseer una múltiples características representadas por sus columnas\n",
    "- Cada característica puede ser de tipo:\n",
    "    * Numérica: Característica cuantitativa\n",
    "    * Nominales: Característica conformada por clases no ordenadas\n",
    "    * Ordinales: Característica conformada por clases ordenadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0520359",
   "metadata": {},
   "source": [
    "# Transformación de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a8abb",
   "metadata": {},
   "source": [
    "- Normalmente los modelos de Machine Learning tienen problemas para trabajar con características no numéricas.\n",
    "- Este tipo de características deben ser transformadas:\n",
    "    * Ordinales: Reemplazar clases por números ordenados\n",
    "    * Nominales: One Hot Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411dc845",
   "metadata": {},
   "source": [
    "# Transformación de características: ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3984c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación ordinal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Cargamos los datos\n",
    "df = pd.DataFrame(['bad', 'good', 'good', 'average','bad','average', 'good'])\n",
    "# Creamos un objeto OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder(categories=[['bad', 'average', 'good']])\n",
    "ordinal_encoder.fit(df) # Estimamos los parámetros internos con fit()\n",
    "serie_encoded = ordinal_encoder.transform(df) # Transformamos los datos de df a números\n",
    "print(\"Datos originales\", df)\n",
    "print(\"Serie codificada\", serie_encoded)\n",
    "print(\"Categorías\", ordinal_encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80520584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El reemplazo ordinal no siempre es la mejor opción\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(['red', 'blue', 'blue', 'green','red','green', 'red'])\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "ordinal_encoder.fit(df)\n",
    "serie_encoded = ordinal_encoder.transform(df)\n",
    "print(\"Datos originales\", df)\n",
    "print(\"Serie codificada\", serie_encoded)\n",
    "print(\"Categorías\", ordinal_encoder.categories_)\n",
    "# ¿Tiene sentido que el azul sea menor que el verde?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810cee2",
   "metadata": {},
   "source": [
    "# Transformación de características: One Hot Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0320d",
   "metadata": {},
   "source": [
    "- One Hot Encoder codifica las categorías de una columna de datos, como una nueva columna por cada categoría.\n",
    "- Cuando un dato posee una cierta categorías, se marca su respectiva columna con un 1 y el resto de columnas con cero.\n",
    "- Esto asegura que todos las categorías tengan la misma distancia entre sí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(['red', 'blue', 'blue', 'green','red','green', 'red'])\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "one_hot_encoder.fit(df) # Aprendemos la codificación one hot a partir de nuestros datos\n",
    "serie_encoded = one_hot_encoder.transform(df) # Transformamos los datos según lo aprendido\n",
    "print(\"Datos originales\", df)\n",
    "print(\"Serie codificada\", serie_encoded.toarray())\n",
    "print(\"Categorías\", one_hot_encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7756be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando a dataframe\n",
    "df_encoded = pd.DataFrame(serie_encoded.toarray(), columns=one_hot_encoder.categories_)\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983cbe5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Escalamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfec30e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Modificación del rango de valores de una característica numérica\n",
    "- Tipos comunes de escalamiento:\n",
    "    * Escalamiento min-max (normalización): Rango entre 0 (mínimo) y 1 (máximo)\n",
    "    * Estandarización: Escalamiento a media cero y varianza unitaria\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f806d9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Estandarización con Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f2f0a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/housing.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494394d3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Estandarización con Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22ea58",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Esta biblioteca se encarga de el escalamiento por estandarización\n",
    "# Es decir, bajo el supuesto que los datos se comportan como una \n",
    "# distribución normal\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler() # Creación de objeto StandardScaler\n",
    "scaler.fit(df[['median_house_value']]) # Cálculo de parámetros de escalamiento\n",
    "print('Media de los datos', scaler.mean_)\n",
    "print('Varianza de los datos', scaler.var_)\n",
    "price_scaled = scaler.transform(df[['median_house_value']]) # Transformación de los datos\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(price_scaled, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c519919",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Estandarización con Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d64c47",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.hist(df['median_house_value'], bins=20)\n",
    "ax1.set_title('Precios reales')\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.hist(price_scaled, bins=20)\n",
    "ax2.set_title('Precios escalados')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16c351",
   "metadata": {},
   "source": [
    "# Column Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5178338",
   "metadata": {},
   "source": [
    "- Podemos paralelizar el proceso de transformación de múltiples columnas de datos de distintos tipos utilizando Column Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "df = pd.read_csv('data/housing.csv')[['median_income', 'median_house_value', 'ocean_proximity']]\n",
    "df.info()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1264af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectores de columnas por tipos. También es posible utilizar patrones de expresiones\n",
    "# regulares para selección de columnas\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html\n",
    "cat_selector = make_column_selector(dtype_include='object')\n",
    "num_selector = make_column_selector(dtype_include='number')\n",
    "\n",
    "# Transformadores\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "standar_scaler = StandardScaler()\n",
    "\n",
    "# Transformación\n",
    "col_transformer = make_column_transformer((one_hot_encoder, cat_selector), \n",
    "                        (standar_scaler, num_selector),\n",
    "                        remainder = 'passthrough') # Si existe alguna columna que no caiga en las categorías, pasa tal cual\n",
    "\n",
    "df_encoded = pd.DataFrame(col_transformer.fit_transform(df))\n",
    "df_encoded\n",
    "# También es posible utilizar la clase ColumnTransform la cual es mucho más versátil\n",
    "# que la función make_column_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7417d",
   "metadata": {},
   "source": [
    "# Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4a70c3",
   "metadata": {},
   "source": [
    "- SimpleImputer es una clase de Scikit-Learn que permite el manejo sencillo de columnas de características con valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366db8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df = pd.read_csv('data/housing.csv')\n",
    "df_num = df.drop(\"ocean_proximity\", axis=1) # borrando columna no numérica\n",
    "df_num.info()\n",
    "# Valores nulos en columna total_bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e64ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(df_num)\n",
    "print(imputer.statistics_) # valores de mediana por cada columna numérica\n",
    "df_tr = pd.DataFrame(imputer.transform(df_num), columns=df_num.columns, index=df_num.index)\n",
    "df_tr.info() # Valores nulos reemplazados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3afd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eef93e",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9051b",
   "metadata": {},
   "source": [
    "- A diferencia de los ColumnTransformers, los Pipeline pueden realizar múltiples operaciones sobre un dataset **en secuencia**.\n",
    "- Los pipelines pueden incluir modelos como operaciones compatibles y otros elementos de Scikit-Learn siempre y cuando ellos cuenten con los métodos *fit* y *transform*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Ojo, las siguientes dos líneas son opcionales\n",
    "# Permiten ver visualmente el pipeline creado\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram') # Para mostrar como diagrama\n",
    "\n",
    "# Datos estadísticos sobre diferentes países\n",
    "df = pd.read_csv('data/life_expectancy.csv', index_col='CountryYear')\n",
    "df.info()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce56d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler = StandardScaler()\n",
    "# Creación de pipeline a partir de sus componentes. En\n",
    "# este ejemplo, el imputer y el scaler\n",
    "preprocessing_pipeline = make_pipeline(imputer, scaler)\n",
    "preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9fea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.DataFrame(preprocessing_pipeline.fit_transform(df),\n",
    "                            columns=df.columns, index=df.index)\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b4936a",
   "metadata": {},
   "source": [
    "# División en set de training y test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee8e7c",
   "metadata": {},
   "source": [
    "- Los modelos de Machine Learning intentan aprender patrones en los datos que le permitan hacer predicciones.\n",
    "- Al momento de evaluar un modelo, se desea estimar qué tan bien es capaz de **generalizar** a partir de los datos usados en el proceso de aprendizaje. (¿Puedo hacer predicciones sobre nuevos datos a partir de lo ya aprendido?)\n",
    "- Para ello, siempre es necesario dividir nuestro data set en dos grupos:\n",
    "    * **Training set**: Es el subconjunto de datos que será utilizado para entrenar nuestro modelo.\n",
    "    * **Test set**: Es el subconjunto de datos que será utilizado para evaluar la generalización de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f163cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de división de datos de entrenamiento y de prueba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datos estadísticos sobre diferentes países\n",
    "df = pd.read_csv('data/life_expectancy.csv', index_col='CountryYear')\n",
    "X = df.drop([\"Life expectancy\"], axis=1) # Features\n",
    "y = df[\"Life expectancy\"] # Target\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca38f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.info()) # Set de características de datos de training\n",
    "print(X_test.info()) # Set de características de datos de test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba661a96",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Actividad 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847f5ef",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Utilice como base el dataset [housingprice.csv](data/housingprice.csv). Este dataset incluye información variada de distintas casas junto con sus valores de venta (columna de interés). \n",
    "- Realice sobre dicho dataset las transformaciones que considere pertitentes, incluyendo:\n",
    "    * Tratamiento de valores nulos\n",
    "    * Estandarización de valores numéricos\n",
    "    * Transformación de categorías ordinales y nominales\n",
    "    * División en datos de entrenamiento y prueba.\n",
    "- Puede encontrar información adicional sobre cada columna [aquí](data/data_description_housing_price.txt) e información adicional sobre el set de datos [aquí](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
